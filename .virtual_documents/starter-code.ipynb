








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error



!    pip install -U scikit-learn





df = pd.read_csv('data/College.csv')


df = df.rename({'Unnamed: 0' : 'University'}, axis = 1)





df.columns = df.columns.str.lower().str.replace('.', '_')


df.columns








# Check for nulls
df.isna().sum()


# Check column data types
df.dtypes


# Hmm... PHD
df['phd'].value_counts()





# method 1- turning a column into a numerical column 'with "?"'

df['phd'] = pd.to_numeric(df['phd'],errors = 'coerce')

# method 2

# df['phd'] = df['phd'].replace('?', np.NaN).astype(float)


df.isna().sum()








df = df.dropna()


df.shape











# df['private'] = df['private'].replace(to_replace=['Yes','No'], value= [1,0])
# df['private'].replace({'Yes' : 1, 'No' : 0})

# methoid 2
df['private'] = (df['private'] == 'Yes').astype(int)

#methoid 3
# df['private'].map({'Yes': 1, 'No': 0})


df['private'].value_counts()


df.dtypes








# Box plot
sns.boxplot(df, x = 'apps')


# Histogram
sns.histplot(df, x = 'apps')





df.loc[df['apps'] > 40000]


# We can make a decision to drop the outlier, if we want! Test it!
df = df.loc[df['apps'] <= 40000]


# Box plot
sns.boxplot(df, x = 'apps')


# Histogram
sns.histplot(df, x = 'apps')





df.describe()


sns.histplot(df, 
            x = 'phd')


df.loc[df['phd'] > 100]


sns.histplot(df, 
            x = 'grad_rate')


df.loc[df['grad_rate'] > 100]


df['grad_rate'] = np.minimum(df['grad_rate'], 100)


df['phd'] = np.minimum(df['phd'], 100)


df.describe()





df.hist(figsize = (15,15))





corr1 = df.corr(numeric_only= True)


plt.figure(figsize = (15,10))
sns.heatmap(corr1, 
           cmap = 'coolwarm', 
           annot = True, 
           vmin = -1, 
           vmax = 1)





plt.figure(figsize=(10,8))
sns.heatmap(df.corr(numeric_only= True)[['apps']].sort_values(by = 'apps', ascending= False), 
           annot = True, 
           vmin= -1, 
           vmax = 1, 
           cmap = 'coolwarm')





sns.pairplot(df, 
             x_vars= ['accept','enroll','f_undergrad','phd', 'perc_alumni'], 
             y_vars= ['apps'])








df.columns


features = ['private', 'top10perc',
       'top25perc', 'f_undergrad', 'p_undergrad', 'outstate', 'room_board',
       'books', 'personal', 'phd', 'terminal', 's_f_ratio', 'perc_alumni',
       'expend', 'grad_rate']

X = df[features]

# X = df[['','','','','']]
# df.drop(columns = [list columns you want to drop])

y = df['apps']


X





X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)


X_train


X_test


y_train


y_test





lr = LinearRegression()


lr.fit(X_train, y_train)


np.set_printoptions(suppress= True)


y_test


lr.predict(X_test)











# Training R2 Score:
lr.score(X_train, y_train)


# Testing R2 Score:
lr.score(X_test, y_test)














# Make predictions on training set
train_preds = lr.predict(X_train)


# Make predictions on test set
test_preds = lr.predict(X_test)


# Calculate RMSE on training set
mse = mean_squared_error(y_train, train_preds)
np.sqrt(mse)


# Calculate RMSE on testing set
mse2 = mean_squared_error(y_test, test_preds)
np.sqrt(mse2)





# Calculate the mean of our target in our test group
y_test.mean()


# Show off np.full_like
baseline_preds = np.full_like(y_test, y_test.mean(), dtype= float)


# Putting it all together -- Calculate the baseline RMSE 
baseline_mse = mean_squared_error(y_test, baseline_preds)
np.sqrt(baseline_mse)











# Coefficients?
lr.coef_


X_train.columns


# Let's make that a bit easier to read
coef_df = pd.DataFrame({
    'Feature' : X_train.columns,
    'Coef' : lr.coef_
})


sorted_df = coef_df.sort_values(by = 'Coef', ascending= False)


sns.barplot(sorted_df, x = 'Coef', y = 'Feature')












