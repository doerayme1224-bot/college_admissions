


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import root_mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error


df = pd.read_csv('data/College.csv')


df.head()


df = df.rename({'Unnamed: 0' : 'University'}, axis = 1)


df.isna().sum()


df.dtypes


df['PhD'] = pd.to_numeric(df['PhD'],errors = 'coerce')


df = df.dropna()


df['Private'] = df['Private'].replace({'Yes' : 1, 'No' : 0})


df.to_csv('data/cleaned_college.csv', index = False)





data = pd.read_csv('data/cleaned_college.csv')


data['Grad.Rate'] = np.minimum(data['Grad.Rate'], 100)


data['PhD'] = np.minimum(data['PhD'], 100)


data.head()


plt.figure(figsize= (6, 12))
sns.heatmap(data.corr(numeric_only= True)[['Apps']].sort_values(by = 'Apps', ascending= False), 
           vmin = -1, 
           vmax = 1, 
           annot = True, 
           cmap = 'coolwarm')





plt.figure(figsize= (6, 12))
sns.heatmap(data[['Accept','Enroll','F.Undergrad']].corr(numeric_only= True)[['Apps']].sort_values(by = 'Apps', ascending= False), 
           vmin = -1, 
           vmax = 1, 
           annot = True, 
           cmap = 'coolwarm')


sns.set_style('darkgrid')


sns.regplot(data,x = 'Grad.Rate',y = 'Outstate')


sns.histplot(data, x = 'Grad.Rate')











X = data[['Accept','Enroll','F.Undergrad']]

y = data['Apps']


scores = []

for i in range(10, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    lr.predict(X_test)
    train_score = lr.score(X_train, y_train)
    test_score = lr.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
lr_scores = pd.DataFrame(scores)


lr_scores
# Linear Regression got the best R squared scores by far, so I'll use it


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.10)
    
lr = LinearRegression()

lr.fit(X_train, y_train)


lr.score(X_test, y_test)





lr_pkl = ''


with open('lr_pkl','wb') as file:
    model = pickle.dump(lr, file)


with open('lr_pkl','rb') as file:
    model = pickle.load(file)








scores = []

for i in range(10, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    rfr = RandomForestRegressor(random_state= 42)
    rfr.fit(X_train, y_train)
    rfr.predict(X_test)
    train_score = rfr.score(X_train, y_train)
    test_score = rfr.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
rfr_scores = pd.DataFrame(scores)


rfr_scores








scores = []

for i in range(10, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    dtr = DecisionTreeRegressor(random_state= 42)
    dtr.fit(X_train, y_train)
    dtr.predict(X_test)
    train_score = dtr.score(X_train, y_train)
    test_score = dtr.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
dtr_scores = pd.DataFrame(scores)


dtr_scores
